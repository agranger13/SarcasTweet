{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ML.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kofr94nxlwlR",
        "outputId": "5209ec50-0b90-41c0-e3a8-268a1742faa8"
      },
      "source": [
        "!pip install elasticsearch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-7.13.4-py2.py3-none-any.whl (356 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 356 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch) (2021.5.30)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.13.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZraJ1PO026LC",
        "outputId": "75ab7227-ab72-4323-ac70-6b0f687d7bbb"
      },
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd \n",
        "import os\n",
        "import re #Regular expression\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import elasticsearch\n",
        "import elasticsearch.helpers\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "PzcPZ8b826LK",
        "outputId": "3e25252f-ed28-40dc-9b73-89641ba2a137"
      },
      "source": [
        "host = 'search-sarcastweet-7w4lvds7cvubzgyd4i4kq72gwe.us-east-2.es.amazonaws'\n",
        "\n",
        "es = elasticsearch.Elasticsearch(\n",
        "    hosts=[\"https://root_user:M9p7r3u*@search-sarcastweet-7w4lvds7cvubzgyd4i4kq72gwe.us-east-2.es.amazonaws.com\"],\n",
        ")\n",
        "\n",
        "es.info()\n",
        "body={\"query\": {\"match_all\": {}}}\n",
        "results = elasticsearch.helpers.scan(es, query=body, index=\"tweets\", request_timeout=30)\n",
        "data = pd.DataFrame.from_dict([document['_source'] for document in results])\n",
        "\n",
        "# Remove unused columns and clean Na\n",
        "data.dropna(subset=[\"Tweet\", \"label\"], inplace=True)\n",
        "\n",
        "# endpoint = 'https://search-projet-final-7oxdpiy44ynvktr43nimfxpjuy.us-east-2.es.amazonaws.com'\n",
        "# results = requests.get(endpoint + '/tweets/_search', auth=('root_user','M9p7r3u*')).json()\n",
        "# print(results['hits'][\"hits\"])\n",
        "# data = pd.DataFrame.from_dict([document[\"_source\"] for document in results['hits'][\"hits\"]])\n",
        "\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1099, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jure embauché dans quoi ?</td>\n",
              "      <td>Non</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vacciné et donc prêt à capter la 5G pour suivr...</td>\n",
              "      <td>Oui</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T’es con ou quoi ? T’as cité mon tweet donc t’...</td>\n",
              "      <td>Non</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Donc tu dors pas tout le long.</td>\n",
              "      <td>Non</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non mais ils sont congolais et rationnel et n'...</td>\n",
              "      <td>Non</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet label text\n",
              "0                          jure embauché dans quoi ?   Non  NaN\n",
              "1  Vacciné et donc prêt à capter la 5G pour suivr...   Oui  NaN\n",
              "2  T’es con ou quoi ? T’as cité mon tweet donc t’...   Non  NaN\n",
              "3                     Donc tu dors pas tout le long.   Non  NaN\n",
              "4  Non mais ils sont congolais et rationnel et n'...   Non  NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v3faSlf626LM",
        "outputId": "691ed88b-a775-415f-e01b-7b39c5416c47"
      },
      "source": [
        "data.loc[data['label'] == \"Oui\", 'sarcastic'] = 1\n",
        "data.loc[data['label'] == \"Non\", 'sarcastic'] = 0\n",
        "data['sarcastic'] = data[\"sarcastic\"].astype(int)\n",
        "\n",
        "data = data.drop(columns=\"label\")\n",
        "data = data.drop(columns=\"text\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jure embauché dans quoi ?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vacciné et donc prêt à capter la 5G pour suivr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>T’es con ou quoi ? T’as cité mon tweet donc t’...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Donc tu dors pas tout le long.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non mais ils sont congolais et rationnel et n'...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet  sarcastic\n",
              "0                          jure embauché dans quoi ?          0\n",
              "1  Vacciné et donc prêt à capter la 5G pour suivr...          1\n",
              "2  T’es con ou quoi ? T’as cité mon tweet donc t’...          0\n",
              "3                     Donc tu dors pas tout le long.          0\n",
              "4  Non mais ils sont congolais et rationnel et n'...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56NIJHJU26LN",
        "outputId": "099c1133-c773-4215-8066-aa0ab88679ec"
      },
      "source": [
        "def CleanTokenize(df):\n",
        "    head_lines = list()\n",
        "    lines = df[\"Tweet\"].values.tolist()\n",
        "\n",
        "    for line in lines:\n",
        "        text = line.lower()\n",
        "        emoji = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticones\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symboles \n",
        "                           u\"\\U0001F680-\\U0001F6FF\" \n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # drapeau (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        text = text.lower()\n",
        "        text = emoji.sub(r'', text)\n",
        "        text = re.sub(r\" vs \", \"vous\", text)\n",
        "        text = re.sub(r\" slt \", \"salut\", text)\n",
        "        text = re.sub(r\" stp \", \"s'il te plaît\", text)\n",
        "        text = re.sub(r\" mm \", \"meme\", text) \n",
        "        text = re.sub(r\" mtn \", \"maintenant\", text)\n",
        "        text = re.sub(r\" vrm \", \"vraiment\", text)\n",
        "        text = re.sub(r\" tt \", \"tout\", text)\n",
        "        text = re.sub(r\" pq \", \"pourquoi\", text)\n",
        "        text = re.sub(r\" pk \", \"pourquoi\", text)\n",
        "        text = re.sub(r\" tkt \", \"t inquiete\", text)\n",
        "        text = re.sub(r\" tqt \", \"t inquiete\", text)\n",
        "        text = re.sub(r\" t \", \"tu es\", text)  \n",
        "        text = re.sub(r\" tas \", \"tu as\", text) \n",
        "        text = re.sub(r\" t'as \", \"tu as\", text) \n",
        "        text = re.sub(r\" t'es \", \"tu es\", text) \n",
        "        text = re.sub(r\" t'est \", \"tu es\", text) \n",
        "        text = re.sub(r\" maie \", \"mais\", text)\n",
        "        text = re.sub(r\" mai \", \"mais\", text)\n",
        "        text = re.sub(r\" y'a \", \"il y a\", text)\n",
        "        text = re.sub(r\" ya \", \"il y a\", text)\n",
        "        text = re.sub(r\" j \", \"je\", text)\n",
        "        text = re.sub(r\" j ai \", \"j'ai\", text)\n",
        "        text = re.sub(r\" l \", \"l'\", text)\n",
        "        text = re.sub(r\" c  \", \"c'est\", text)\n",
        "        text = re.sub(r\" bcp \", \"beaucoup\", text)\n",
        "        text = re.sub(r\" jvais \", \"je vais\", text)\n",
        "        text = re.sub(r\" jpense \", \"je pense\", text)\n",
        "        text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
        "        text = re.sub(r\"’\", \"\", text)\n",
        "        text = re.sub(r\"»\", \"\", text)\n",
        "        tokens = word_tokenize(text)\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        stop_words = set(stopwords.words(\"french\"))\n",
        "        words = [w for w in stripped if not w in stop_words]\n",
        "        head_lines.append(words)\n",
        "    return head_lines\n",
        "\n",
        "head_lines = CleanTokenize(data)\n",
        "head_lines[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['jure', 'embauché', 'quoi'],\n",
              " ['vacciné',\n",
              "  'donc',\n",
              "  'prêt',\n",
              "  'capter',\n",
              "  '5g',\n",
              "  'suivre',\n",
              "  'direct',\n",
              "  'wwdc21',\n",
              "  'dapple',\n",
              "  'dès',\n",
              "  'soir',\n",
              "  '19h00',\n",
              "  'plein',\n",
              "  'dannonces',\n",
              "  'softwares',\n",
              "  'hardwares',\n",
              "  'perspective',\n",
              "  'récapitulées',\n",
              "  'petites',\n",
              "  'mains',\n",
              "  'thread',\n",
              "  'juste',\n",
              "  'dessous',\n",
              "  'présent',\n",
              "  'tweet',\n",
              "  '•',\n",
              "  'pronostics'],\n",
              " ['con',\n",
              "  'quoi',\n",
              "  'tas',\n",
              "  'cité',\n",
              "  'tweet',\n",
              "  'donc',\n",
              "  'tas',\n",
              "  'bien',\n",
              "  'vu',\n",
              "  'japprécie',\n",
              "  'zuukou',\n",
              "  'comme',\n",
              "  'jai',\n",
              "  'quon',\n",
              "  'appelle',\n",
              "  'cerveau',\n",
              "  'capable',\n",
              "  'juger',\n",
              "  'quels',\n",
              "  'sons',\n",
              "  'plaisent',\n",
              "  'non',\n",
              "  'exprimer',\n",
              "  'pourquoi',\n",
              "  'maintenant',\n",
              "  'baises',\n",
              "  'mère',\n",
              "  'mentions',\n",
              "  'petit',\n",
              "  'dep'],\n",
              " ['donc', 'dors', 'tout', 'long'],\n",
              " ['non',\n",
              "  'congolais',\n",
              "  'rationnel',\n",
              "  'nont',\n",
              "  'besoin',\n",
              "  'dexclure',\n",
              "  'autres',\n",
              "  'sentir',\n",
              "  'plus',\n",
              "  'congolais',\n",
              "  'simple',\n",
              "  'aujourdhui',\n",
              "  'être',\n",
              "  'natif',\n",
              "  'dun',\n",
              "  'père',\n",
              "  'mère',\n",
              "  'étranger',\n",
              "  'devient',\n",
              "  'crime',\n",
              "  'heureusement',\n",
              "  'linstant',\n",
              "  'lattaque',\n",
              "  'diriger',\n",
              "  'issue',\n",
              "  'dun',\n",
              "  'mélange'],\n",
              " ['genre', 'avraimentquelqu', 'a', 'demander', 'ça'],\n",
              " ['croyais',\n",
              "  'tout',\n",
              "  'temps',\n",
              "  'papacito',\n",
              "  'probablement',\n",
              "  'tube',\n",
              "  'lété',\n",
              "  'crétin',\n",
              "  'échappé',\n",
              "  'oreilles',\n",
              "  'bizarrement',\n",
              "  'préfère',\n",
              "  'rester',\n",
              "  'cette',\n",
              "  'idée'],\n",
              " ['ibra',\n",
              "  'a',\n",
              "  'jamais',\n",
              "  'marqué',\n",
              "  'lhistoire',\n",
              "  'dune',\n",
              "  'grande',\n",
              "  'compétition',\n",
              "  'lol',\n",
              "  'bientôt',\n",
              "  'ça',\n",
              "  'va',\n",
              "  'mettre',\n",
              "  'ben',\n",
              "  'arfa',\n",
              "  'top',\n",
              "  '100',\n",
              "  'histoire',\n",
              "  'sous',\n",
              "  'prétexte',\n",
              "  'quils',\n",
              "  'bonne',\n",
              "  'conduite',\n",
              "  'balle',\n",
              "  'si',\n",
              "  'cest',\n",
              "  'ça',\n",
              "  'connaître',\n",
              "  'foot',\n",
              "  'préfère',\n",
              "  'rester',\n",
              "  'lignorance'],\n",
              " ['jutilise',\n",
              "  'peu',\n",
              "  'trop',\n",
              "  'souvent',\n",
              "  '«',\n",
              "  'bloquer',\n",
              "  'commentaires',\n",
              "  'x',\n",
              "  'oups'],\n",
              " ['thread', 'stars', 'illuminati', 'reporté', 'semaine', 'pro']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wKAZapr26LS",
        "outputId": "95043f5f-d490-4e0f-c28b-27f94fb5ff3a"
      },
      "source": [
        "validation_split = 0.2\n",
        "max_length = 25\n",
        "\n",
        "\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(head_lines)\n",
        "sequences = tokenizer_obj.texts_to_sequences(head_lines)\n",
        "\n",
        "word_index = tokenizer_obj.word_index\n",
        "print(\"unique tokens - \",len(word_index))\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "print('vocab size -', vocab_size)\n",
        "\n",
        "lines_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "sentiment =  data['sarcastic'].values\n",
        "\n",
        "indices = np.arange(lines_pad.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "lines_pad = lines_pad[indices]\n",
        "sentiment = sentiment[indices]\n",
        "\n",
        "num_validation_samples = int(validation_split * lines_pad.shape[0])\n",
        "\n",
        "X_train_pad = lines_pad[:-num_validation_samples]\n",
        "y_train = sentiment[:-num_validation_samples]\n",
        "X_test_pad = lines_pad[-num_validation_samples:]\n",
        "y_test = sentiment[-num_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique tokens -  5428\n",
            "vocab size - 5429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Shu7IPEB26LU",
        "outputId": "398ba9a6-f53e-44fc-d202-38895960f08c"
      },
      "source": [
        "vector_rep = {}\n",
        "dimention = 100\n",
        "f = open(os.path.join('/content/sample_data/glove.twitter.27B.100d.txt'), encoding = \"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    vector_rep[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(vector_rep))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7ba7f241d2ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvector_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdimention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/glove.twitter.27B.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/glove.twitter.27B.100d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwTO-6XF26LX"
      },
      "source": [
        "matrix = np.zeros((len(word_index) + 1, dimention))\n",
        "c = 0\n",
        "for word, i in word_index.items():\n",
        "    vector = vector_rep.get(word)\n",
        "    if vector is not None:\n",
        "        c+=1\n",
        "        matrix[i] = vector\n",
        "print(c)\n",
        "layer = Embedding(len(word_index) + 1,\n",
        "                  dimention,\n",
        "                  weights=[matrix],\n",
        "                  input_length=max_length,\n",
        "                  trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56cEyzTA26La"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layer)\n",
        "model.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.15 , dropout = 0.15)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "model.fit(X_train_pad, y_train, batch_size=32, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwvykJXe26Ld"
      },
      "source": [
        "def is_ironique(s):\n",
        "    recup_data = pd.DataFrame({\"Tweet\":[s]})\n",
        "    test_lignes = CleanTokenize(recup_data)\n",
        "    test_sequences = tokenizer_obj.texts_to_sequences(test_lignes)\n",
        "    test_review = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
        "    prediction = model.predict(test_review)\n",
        "    prediction*=100\n",
        "    return prediction[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfKxKc7z26Ld"
      },
      "source": [
        "is_ironique(\"J'aime bien ce film.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANgd4t3d26Le"
      },
      "source": [
        "is_ironique(\"Genial, encore un mec bizarre\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxHjcdDh26Lf"
      },
      "source": [
        "is_ironique(\"go faire ma 2eme dose, a moi la 5G\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhBpY-9m26Lg"
      },
      "source": [
        "is_ironique(\"T'as un cerveau ?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a7_v-D-26Lg"
      },
      "source": [
        "is_ironique(\"Merci d'avoir détruit mon colis après l'avoir bien emballé\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXDfiewv26Li"
      },
      "source": [
        "is_ironique(\"Bon go me faire mettre la 5G sur mon bras\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}